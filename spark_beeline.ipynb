{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvsmFxMJYjY1",
        "outputId": "9329d8e7-1dae-408a-c6eb-337605f3341e"
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 66 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 51.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=2a1dfb56be44176604c1b7bc808de7c89f609f6ec520a400969f94d475c281d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH5aV7pzZ7aE"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master('local[4]')\\\n",
        "        .appName('beeline_test')\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .config('spark.executor.instances', 2)\\\n",
        "        .config('spark.executor.memory', '5g')\\\n",
        "        .config('spark.executor.cores', 2)\\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiRu_04daBoj"
      },
      "source": [
        "# Считываем данные, прописываем хэдеры.\n",
        "df_customer = spark.read.csv('customer.csv', sep='\\t').toDF('customer_id', 'name', 'email', 'joinDate', 'status')\n",
        "df_order = spark.read.csv('order.csv', sep='\\t').toDF('customer_id', 'order_id', 'product_id', 'numberOfProduct', 'orderDate', 'Status')\n",
        "df_product = spark.read.csv('product.csv', sep='\\t').toDF('product_id', 'product_name', 'price', 'numberOfProducts')"
      ],
      "execution_count": 553,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPQF8thek2z8"
      },
      "source": [
        "# Объядиняем таблицы\n",
        "df_result = df_order.join(df_customer, 'customer_id', how='left').join(df_product, 'product_id', how='inner')"
      ],
      "execution_count": 554,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUYt1BwGr3fw"
      },
      "source": [
        "# Группируем по названию продукта и имени пользователя. Как метрику пополярности взял сумму купленных товаров по определенной позиции. Также можно было посчитать и по количеству\n",
        "df_result = df_result.groupBy('product_name', 'name')\\\n",
        "                     .agg(F.sum('numberOfProduct').alias('total_buys'))"
      ],
      "execution_count": 555,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GDhP3CAD8S5"
      },
      "source": [
        "# Добавляем ранг по именам пользователей для последующей выборки самой популярной позиции.\n",
        "# Использовал row_number() тк при использовании dense_rank() в выборке получалось несколько популярных значений в связи с равным объемом покупок\n",
        "window_spec = Window\\\n",
        "              .partitionBy('name')\\\n",
        "              .orderBy(result.total_buys.desc())\n",
        "\n",
        "result = result.withColumn('rank', F.row_number().over(window_spec))\n",
        "# Фильтруем по рангу \n",
        "result_to_save = result.select('product_name', 'name').where(result.rank == 1)"
      ],
      "execution_count": 556,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrllOYeSpou8",
        "outputId": "81119aea-d5b2-4b2f-9d34-6898c80677f6"
      },
      "source": [
        "result_to_save.show()"
      ],
      "execution_count": 559,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+---------+\n",
            "|     product_name|     name|\n",
            "+-----------------+---------+\n",
            "|   Apple iPhone 8|Anastasia|\n",
            "|   Apple iPhone 7|     John|\n",
            "|   Apple iPhone 7|   Vasili|\n",
            "|   Apple iPhone 8|   Philip|\n",
            "|Apple iPad mini 4|   Robert|\n",
            "|    Apple AirPods|     Sara|\n",
            "+-----------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mwhl-TXYPzm"
      },
      "source": [
        "# Сохраняем в csv\n",
        "result_to_save.write.csv('bestsellers.csv', sep=',', header=True, encoding='cp1251')"
      ],
      "execution_count": 558,
      "outputs": []
    }
  ]
}